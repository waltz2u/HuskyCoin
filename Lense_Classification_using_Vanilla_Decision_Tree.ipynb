{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lense Classification using Decision Tree.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0jtJQ55vtE4uKxkGUbkDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltz2u/HuskyCoin/blob/master/Lense_Classification_using_Vanilla_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnn5Pjz9k1nC",
        "outputId": "646b41da-f211-4191-8dc8-453941653d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v3>1\n",
            "=leaf: classify to class=3 with class distribution [{3: 12}]\n",
            "=v2>1\n",
            "==v0>1\n",
            "===leaf: classify to class=2 with class distribution [{2: 2}]\n",
            "===v0>2\n",
            "====leaf: classify to class=2 with class distribution [{2: 2}]\n",
            "====v1>1\n",
            "=====leaf: classify to class=3 with class distribution [{3: 1}]\n",
            "=====leaf: classify to class=2 with class distribution [{2: 1}]\n",
            "==v1>1\n",
            "===leaf: classify to class=1 with class distribution [{1: 3}]\n",
            "===v0>1\n",
            "====leaf: classify to class=1 with class distribution [{1: 1}]\n",
            "====leaf: classify to class=3 with class distribution [{3: 2}]\n",
            "Train Accuracy: 62.500000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################\n",
        "# Decision Trees #\n",
        "##################\n",
        "import numpy as np\n",
        "\n",
        "class decision_tree:\n",
        "    def __init__(self):\n",
        "        self.split_value = 0\n",
        "        self.split_attr = -1\n",
        "        self.max_gain = 0.1\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.depth = 0\n",
        "        self.distribution = []\n",
        "\n",
        "def entropy(data, target_attr):\n",
        "    # Calculates the entropy of the given data set for the target attribute.\n",
        "    val_freq = {}\n",
        "    data_entropy = 0.0\n",
        " \n",
        "    # Calculate the frequency of each of the values in the target attr\n",
        "    for record in data:\n",
        "        if (record[target_attr]) in val_freq.keys():\n",
        "            val_freq[record[target_attr]] += 1.0\n",
        "        else:\n",
        "            val_freq[record[target_attr]]  = 1.0\n",
        " \n",
        "    # Calculate the entropy of the data for the target attribute\n",
        "    for freq in val_freq.values():\n",
        "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
        "        \n",
        "    return data_entropy\n",
        "\n",
        "def gain(data, attr, target_attr):\n",
        "    # Calculates the information gain (reduction in entropy) that would result by splitting the data on the chosen attribute (attr).\n",
        "    val_freq = {}\n",
        "    subset_entropy = 0.0\n",
        " \n",
        "    # Calculate the frequency of each of the values in the target attribute\n",
        "    for record in data:\n",
        "        if (record[attr] in val_freq):\n",
        "            val_freq[record[attr]] += 1.0\n",
        "        else:\n",
        "            val_freq[record[attr]]  = 1.0\n",
        " \n",
        "    # Calculate the sum of the entropy for each subset of records weighted by their probability of occuring in the training set.\n",
        "    for val in val_freq.keys():\n",
        "        val_prob = val_freq[val] / sum(val_freq.values())\n",
        "        data_subset = [record for record in data if record[attr] == val]\n",
        "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
        " \n",
        "    # Subtract the entropy of the chosen attribute from the entropy of the whole data set with respect to the target attribute (and return it)\n",
        "    return (entropy(data, target_attr) - subset_entropy)\n",
        "\n",
        "def growTree(node, data,target_attr):\n",
        "    # which attribute and value is best to split\n",
        "    for i in range(len(data)): # go over each instance to split to left/right\n",
        "        for j in range(len(data[0])): # in each instance, go over each attribute\n",
        "            if gain(data,j,target_attr) > node.max_gain and j != target_attr:\n",
        "                node.split_value = data[i][j]\n",
        "                node.split_attr = j\n",
        "                node.max_gain = gain(data,j,target_attr)\n",
        "\n",
        "    if node.split_attr != -1:           \n",
        "        \n",
        "        print(\"=\" * node.depth + \"v\" + str(node.split_attr) + \">\" + str(node.split_value))\n",
        "                \n",
        "        # now split data\n",
        "        left_data, right_data = [], []\n",
        "        for i in range(len(data)):\n",
        "            if data[i][node.split_attr] > node.split_value:\n",
        "                right_data.append(data[i])\n",
        "            else:\n",
        "                left_data.append(data[i])\n",
        "        node.distribution = {i:[row[-1] for row in data].count(i) for i in set([row[-1] for row in data])}\n",
        "    if node.split_attr == -1 or len(data) <2 or len(right_data) <1 or len(left_data) <1:\n",
        "        # no more splitting\n",
        "        node.leaf = True\n",
        "        node.distribution = {i:[row[-1] for row in data].count(i) for i in set([row[-1] for row in data])}\n",
        "        # max(dic, key=dic.get) returns the key that has max value in a dict namely dic\n",
        "        print(\"=\" * node.depth + \"leaf: classify to class=\" + str(max(node.distribution, key=node.distribution.get)) + \" with class distribution \" + \"[\" + str(node.distribution) + \"]\")\n",
        "        return node\n",
        "\n",
        "    node.left = decision_tree()\n",
        "    node.left.depth = node.depth + 1\n",
        "    node.right = decision_tree()\n",
        "    node.right.depth = node.depth + 1\n",
        "    growTree(node.left, left_data, target_attr)\n",
        "    growTree(node.right, right_data, target_attr)\n",
        "\n",
        "def classify(node, X):\n",
        "    # classify test instances in X given a tree\n",
        "    n = np.size(X, 0)\n",
        "    pred = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        while True:\n",
        "            if not(node.left == None and node.right == None):\n",
        "                if X[i][node.split_attr] >= node.split_value: # if the value of the ith instance is >= split value, \n",
        "                    node = node.right\n",
        "                else:\n",
        "                    node = node.left\n",
        "            else:\n",
        "                #if is_int(max(node.distribution, key=node.distribution.get)):\n",
        "                #    pred[i] = random.choice(set(node.distribution)) #random choice if distribution is equal\n",
        "                #else:\n",
        "                pred[i] = max(node.distribution, key=node.distribution.get)\n",
        "                break\n",
        "    return(pred)\n",
        "    \n",
        "import math\n",
        "# Lenses data https://archive.ics.uci.edu/ml/machine-learning-databases/lenses/lenses.data\n",
        "data = [[1, 1, 1, 1, 1, 3],\n",
        "[2, 1, 1, 1, 2, 2],\n",
        "[3, 1, 1, 2, 1, 3],\n",
        "[4, 1, 1, 2, 2, 1],\n",
        "[5, 1, 2, 1, 1, 3],\n",
        "[6, 1, 2, 1, 2, 2],\n",
        "[7, 1, 2, 2, 1, 3],\n",
        "[8, 1, 2, 2, 2, 1],\n",
        "[9, 2, 1, 1, 1, 3],\n",
        "[10, 2, 1, 1, 2, 2],\n",
        "[11, 2, 1, 2, 1, 3],\n",
        "[12, 2, 1, 2, 2, 1],\n",
        "[13, 2, 2, 1, 1, 3],\n",
        "[14, 2, 2, 1, 2, 2],\n",
        "[15, 2, 2, 2, 1, 3],\n",
        "[16, 2, 2, 2, 2, 3],\n",
        "[17, 3, 1, 1, 1, 3],\n",
        "[18, 3, 1, 1, 2, 3],\n",
        "[19, 3, 1, 2, 1, 3],\n",
        "[20, 3, 1, 2, 2, 1],\n",
        "[21, 3, 2, 1, 1, 3],\n",
        "[22, 3, 2, 1, 2, 2],\n",
        "[23, 3, 2, 2, 1, 3],\n",
        "[24, 3, 2, 2, 2, 3]]\n",
        "\n",
        "tree = decision_tree()\n",
        "growTree(tree,[row[1:] for row in data],4)\n",
        "tree\n",
        "\n",
        "trainpred = classify(tree, data)\n",
        "yTrain = [row[-1] for row in data]\n",
        "print(\"Train Accuracy: %f\\n\" % (np.mean(yTrain == trainpred) * 100))"
      ]
    }
  ]
}